{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f600763",
   "metadata": {},
   "source": [
    "## Saneamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b88f17",
   "metadata": {},
   "source": [
    "### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29141fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from joblib import Parallel, delayed\n",
    "from surprise import Dataset, Reader, SVDpp, accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "import scipy.sparse as sp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0578fed",
   "metadata": {},
   "source": [
    "### Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4d601a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratings data shape: (100000, 4)\n",
      "Users data shape: (943, 5)\n",
      "Movies data shape: (1682, 21)\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your local data folder\n",
    "data_path = 'ml-100k'\n",
    "\n",
    "# Check if the folder exists (opcional)\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"The data folder {data_path} does not exist.\")\n",
    "\n",
    "# Load ratings: user id | item id | rating | timestamp\n",
    "ratings_df = pd.read_csv(\n",
    "    os.path.join(data_path, 'u.data'),\n",
    "    sep='\\t',\n",
    "    names=['UserID', 'MovieID', 'Rating', 'Timestamp'],\n",
    "    engine='python'\n",
    ")\n",
    "\n",
    "# Load users: user id | age | gender | occupation | zip code\n",
    "users_df = pd.read_csv(\n",
    "    os.path.join(data_path, 'u.user'),\n",
    "    sep='|',\n",
    "    names=['UserID', 'Age', 'Gender', 'Occupation', 'Zip-code'],\n",
    "    engine='python'\n",
    ")\n",
    "\n",
    "# Define genre columns in the same order as in the u.item file\n",
    "genre_cols = [\n",
    "    'unknown', 'Action', 'Adventure', 'Animation', \"Children's\", 'Comedy',\n",
    "    'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror',\n",
    "    'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'\n",
    "]\n",
    "\n",
    "# Define full column names for u.item\n",
    "movie_cols = ['MovieID', 'Title', 'ReleaseDate', 'VideoReleaseDate', 'IMDbURL'] + genre_cols\n",
    "\n",
    "# Load movies and keep only MovieID, Title, and genre columns\n",
    "movies_df = pd.read_csv(\n",
    "    os.path.join(data_path, 'u.item'),\n",
    "    sep='|',\n",
    "    names=movie_cols,\n",
    "    usecols=['MovieID', 'Title'] + genre_cols,\n",
    "    encoding='latin1',\n",
    "    engine='python'\n",
    ")\n",
    "\n",
    "# Optional: print shape to verify loading\n",
    "print(f\"Ratings data shape: {ratings_df.shape}\")\n",
    "print(f\"Users data shape: {users_df.shape}\")\n",
    "print(f\"Movies data shape: {movies_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8820b890",
   "metadata": {},
   "source": [
    "### Matriz de ratings user-item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbfa1063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>MovieID</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>UserID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the ratings matrix (UserID as rows, MovieID as columns)\n",
    "ratings_matrix = ratings_df.pivot_table(index='UserID', columns='MovieID', values='Rating')\n",
    "\n",
    "# Display a smaller subset to keep it readable (e.g., first 10 rows and columns)\n",
    "subset_matrix = ratings_matrix.iloc[:10, :10]\n",
    "\n",
    "\n",
    "# Show as styled HTML table\n",
    "display(HTML(subset_matrix.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc62a82b",
   "metadata": {},
   "source": [
    "#### MÃ©tricas de EvaluaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ee9cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "\n",
    "def get_test_indices(R_test):\n",
    "    \"\"\"Positions where we have test ratings.\"\"\"\n",
    "    return list(zip(*np.where(~np.isnan(R_test))))\n",
    "\n",
    "def dcg(relevances):\n",
    "    return sum(rel / np.log2(i + 2) for i, rel in enumerate(relevances))\n",
    "\n",
    "\n",
    "# ---------- 1. RMSE & MAE ----------\n",
    "\n",
    "def eval_rmse_mae(R_test, R_pred):\n",
    "    test_idx = get_test_indices(R_test)\n",
    "    y_true = [R_test[u, j] for (u, j) in test_idx]\n",
    "    y_pred = [R_pred[u, j] for (u, j) in test_idx]\n",
    "    rmse = sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    return rmse, mae\n",
    "\n",
    "\n",
    "# ---------- 2. Novelty & Relevance ----------\n",
    "\n",
    "def eval_novelty_relevance(R_train, R_test, R_pred, top_n=20):\n",
    "    \"\"\"\n",
    "    Novelty: avg(1 / (pop_j + 1)) over top-N recommended unknown items.\n",
    "    Relevance: mean true rating (if any) on those top-N per user, then avg over users.\n",
    "    \"\"\"\n",
    "    num_users, num_items = R_train.shape\n",
    "    item_pop = np.sum(~np.isnan(R_train), axis=0)\n",
    "    inv_pop = 1.0 / (item_pop + 1.0)\n",
    "\n",
    "    novelty_u = []\n",
    "    relevance_u = []\n",
    "\n",
    "    for u in range(num_users):\n",
    "        known = ~np.isnan(R_train[u])\n",
    "        unknown_items = np.where(~known)[0]\n",
    "        if unknown_items.size == 0:\n",
    "            continue\n",
    "\n",
    "        k = min(top_n, unknown_items.size)\n",
    "        top_items = unknown_items[np.argsort(R_pred[u, unknown_items])[::-1][:k]]\n",
    "\n",
    "        # Novelty\n",
    "        item_pop = np.sum(~np.isnan(R_train), axis=0)          # users who interacted\n",
    "        P = (item_pop + 1.0) / (num_users + 2.0)              # Laplace smoothing\n",
    "        novelty_u.append(np.mean(-np.log2(P[top_items])))\n",
    "\n",
    "                # Relevance on test ratings (if exist)\n",
    "        rel = [R_test[u, j] for j in top_items if not np.isnan(R_test[u, j])]\n",
    "        if rel:\n",
    "            relevance_u.append(np.mean(rel))\n",
    "\n",
    "    novelty = float(np.mean(novelty_u)) if novelty_u else np.nan\n",
    "    relevance = float(np.mean(relevance_u)) if relevance_u else np.nan\n",
    "    return novelty, relevance\n",
    "\n",
    "\n",
    "\n",
    "# ---------- 3d. Serendipity (real, con baseline 1-5 reescalado a [0,1]) ----------\n",
    "\n",
    "def eval_serendipity(R_train, R_test, R_pred,train_df, top_n=20, tau=3.5):\n",
    "    \"\"\"\n",
    "    Ser(u) = (1/|R_u|) * sum_{i in R_u} (1 - base01(u,i)) * I[r_ui >= tau]\n",
    "    R_u: top-N Ã­tems desconocidos en train para u que sÃ­ tienen rating en test.\n",
    "    base01: baseline de popularidad reescalado a [0,1] (mÃ¡s alto = mÃ¡s esperado).\n",
    "    \"\"\"\n",
    "    # baseline 1..5 (NaN en Ã­tems vistos), mismo que ya tienes\n",
    "    base15 = popularity_baseline(train_df).values.astype(float)\n",
    "\n",
    "    # reescalar a [0,1] donde estÃ¡ definido\n",
    "    base01 = (base15 - 1.0) / 4.0\n",
    "    base01 = np.clip(base01, 0.0, 1.0)\n",
    "\n",
    "   # 3) Aseguramos que todo sea numpy float\n",
    "    R_train = np.asarray(R_train, dtype=float)\n",
    "    R_test  = np.asarray(R_test,  dtype=float)\n",
    "    R_pred  = np.asarray(R_pred,  dtype=float)\n",
    "    m, _ = R_train.shape\n",
    "    unknown_mask = np.isnan(R_train)\n",
    "\n",
    "    user_vals = []\n",
    "    for u in range(m):\n",
    "        cand = np.where(unknown_mask[u])[0]\n",
    "        if cand.size == 0:\n",
    "            continue\n",
    "\n",
    "        k = min(top_n, cand.size)\n",
    "        top = cand[np.argsort(R_pred[u, cand])[::-1][:k]]  # Top_u\n",
    "\n",
    "        has_gt = ~np.isnan(R_test[u, top])  # sÃ³lo con ground truth en test\n",
    "        Ru = top[has_gt]\n",
    "        if Ru.size == 0:\n",
    "            continue\n",
    "\n",
    "        rel = (R_test[u, Ru] >= tau).astype(float)\n",
    "        ser_u = float(np.mean((1.0 - base01[u, Ru]) * rel))\n",
    "        user_vals.append(ser_u)\n",
    "\n",
    "    return float(np.mean(user_vals)) if user_vals else np.nan\n",
    "\n",
    "# ---------- 4. Diversity ----------\n",
    "\n",
    "def compute_item_similarity(R_train):\n",
    "    \"\"\"Cosine similarity between item vectors (using train, NaNs -> 0).\"\"\"\n",
    "    R_filled = np.nan_to_num(R_train, nan=0.0)\n",
    "    return cosine_similarity(R_filled.T)\n",
    "\n",
    "def eval_diversity(R_train, R_pred, item_similarity, top_n=5):\n",
    "    \"\"\"\n",
    "    Diversity(u) = average_{i<j in Top_u} (1 - Sim(i,j)),\n",
    "    with Top_u being top-N items among those unknown in train.\n",
    "    Users with <2 items contribute 0. Returns mean over users.\n",
    "    \"\"\"\n",
    "    if top_n <= 1:\n",
    "        return 0.0  # con 0 o 1 Ã­tem, diversidad es 0 por definiciÃ³n prÃ¡ctica\n",
    "\n",
    "    R_train = np.asarray(R_train, dtype=float)\n",
    "    R_pred  = np.asarray(R_pred,  dtype=float)\n",
    "    S       = np.asarray(item_similarity, dtype=float)\n",
    "\n",
    "    num_users, _ = R_pred.shape\n",
    "    div_scores = []\n",
    "\n",
    "    # reemplaza NaNs en predicciones por -inf para que nunca entren en el top\n",
    "    R_pred_safe = np.where(np.isnan(R_pred), -np.inf, R_pred)\n",
    "\n",
    "    for u in range(num_users):\n",
    "        unknown = np.where(np.isnan(R_train[u]))[0]\n",
    "        k = min(top_n, unknown.size)\n",
    "\n",
    "        if k < 2:\n",
    "            div_scores.append(0.0)\n",
    "            continue\n",
    "\n",
    "        # Top_u: top-k por predicciÃ³n\n",
    "        top_items = unknown[np.argsort(R_pred_safe[u, unknown])[-k:]]\n",
    "\n",
    "        # media sobre pares i<j de (1 - sim)\n",
    "        s = 0.0\n",
    "        cnt = 0\n",
    "        for a in range(k):\n",
    "            for b in range(a + 1, k):\n",
    "                s += 1.0 - S[top_items[a], top_items[b]]\n",
    "                cnt += 1\n",
    "\n",
    "        div_scores.append(s / cnt)\n",
    "\n",
    "    return float(np.mean(div_scores)) if div_scores else np.nan\n",
    "\n",
    "\n",
    "\n",
    "# ---------- 5. nDCG@K ----------\n",
    "\n",
    "def eval_ndcg(R_train, R_test, R_pred, k=20):\n",
    "    \"\"\"\n",
    "    nDCG@k over unknown-in-train items.\n",
    "    Utility: test rating if available, else 0 (as in the cited definition).\n",
    "    \"\"\"\n",
    "    num_users, _ = R_train.shape\n",
    "    ndcgs = []\n",
    "\n",
    "    for u in range(num_users):\n",
    "        unknown_items = np.where(np.isnan(R_train[u]))[0]\n",
    "        if unknown_items.size == 0:\n",
    "            continue\n",
    "\n",
    "        k_u = min(k, unknown_items.size)\n",
    "\n",
    "        # Predicted ranking (DESC) among unknown items\n",
    "        top_items = unknown_items[np.argsort(R_pred[u, unknown_items])[::-1][:k_u]]\n",
    "\n",
    "        # DCG: gains = true test rating if exists else 0\n",
    "        rel_pred = [R_test[u, j] if not np.isnan(R_test[u, j]) else 0.0 for j in top_items]\n",
    "        dcg_u = dcg(rel_pred)\n",
    "\n",
    "        # IDCG: ideal ranking by true utilities on the SAME candidate set, missing -> 0\n",
    "        true_utils = [R_test[u, j] if not np.isnan(R_test[u, j]) else 0.0 for j in unknown_items]\n",
    "        ideal_rel = sorted(true_utils, reverse=True)[:k_u]\n",
    "        idcg_u = dcg(ideal_rel)\n",
    "\n",
    "        if idcg_u > 0:\n",
    "            ndcgs.append(dcg_u / idcg_u)\n",
    "\n",
    "    return float(np.mean(ndcgs)) if ndcgs else np.nan\n",
    "\n",
    "\n",
    "# ---------- 6. Wrapper: one call for everything ----------\n",
    "\n",
    "def evaluate_all_metrics(R_train, R_test, R_pred, train_df,\n",
    "                         topn_nov_rel=20, topn_div=5, k_ndcg=20):\n",
    "    rmse, mae = eval_rmse_mae(R_test, R_pred)\n",
    "    novelty, relevance = eval_novelty_relevance(R_train, R_test, R_pred,\n",
    "                                                top_n=topn_nov_rel)\n",
    "    serendipity = eval_serendipity(R_train, R_test, R_pred, train_df)\n",
    "    item_sim = compute_item_similarity(R_train)\n",
    "    diversity = eval_diversity(R_train, R_pred, item_sim, top_n=topn_div)\n",
    "    ndcg = eval_ndcg(R_train, R_test, R_pred, k=k_ndcg)\n",
    "\n",
    "    return {\n",
    "        \"RMSE\": rmse,\n",
    "        \"MAE\": mae,\n",
    "        \"Novelty\": novelty,\n",
    "        \"Relevance\": relevance,\n",
    "        \"Serendipity\": serendipity,\n",
    "        \"Diversity\": diversity,\n",
    "        \"nDCG@20\": ndcg,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc6a4bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Fold 1\n",
      "âœ… Train ratings: 99055\n",
      "âœ… Test ratings: 945\n",
      "ðŸš« Skipped test users (less than 5 ratings): 0\n",
      "\n",
      "ðŸ”„ Fold 2\n",
      "âœ… Train ratings: 99055\n",
      "âœ… Test ratings: 945\n",
      "ðŸš« Skipped test users (less than 5 ratings): 0\n",
      "\n",
      "ðŸ”„ Fold 3\n",
      "âœ… Train ratings: 99055\n",
      "âœ… Test ratings: 945\n",
      "ðŸš« Skipped test users (less than 5 ratings): 0\n",
      "\n",
      "ðŸ”„ Fold 4\n",
      "âœ… Train ratings: 99060\n",
      "âœ… Test ratings: 940\n",
      "ðŸš« Skipped test users (less than 5 ratings): 0\n",
      "\n",
      "ðŸ”„ Fold 5\n",
      "âœ… Train ratings: 99060\n",
      "âœ… Test ratings: 940\n",
      "ðŸš« Skipped test users (less than 5 ratings): 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "\n",
    "# ratings_matrix: matriz usuarios x pelÃ­culas, ya definida\n",
    "# Convertimos a array para trabajar con Ã­ndices mÃ¡s rÃ¡pido\n",
    "user_ids = ratings_matrix.index.tolist()\n",
    "user_indices = np.arange(len(user_ids))\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Almacenar los folds\n",
    "folds = []\n",
    "\n",
    "for fold_num, (train_user_idx, test_user_idx) in enumerate(kf.split(user_indices)):\n",
    "    print(f\"\\nðŸ”„ Fold {fold_num + 1}\")\n",
    "    \n",
    "    # Ãndices de usuarios\n",
    "    train_users = [user_ids[i] for i in train_user_idx]\n",
    "    test_users = [user_ids[i] for i in test_user_idx]\n",
    "\n",
    "    # Crear copias de la matriz completa\n",
    "    train_matrix = ratings_matrix.copy()\n",
    "    test_matrix = pd.DataFrame(np.nan, index=ratings_matrix.index, columns=ratings_matrix.columns)\n",
    "\n",
    "    skipped_users = 0\n",
    "\n",
    "    for user in test_users:\n",
    "        user_ratings = ratings_matrix.loc[user].dropna()\n",
    "\n",
    "        if len(user_ratings) >= 5:\n",
    "            test_items = random.sample(list(user_ratings.index), 5)\n",
    "            for item in test_items:\n",
    "                # Move rating to test matrix\n",
    "                test_matrix.at[user, item] = ratings_matrix.at[user, item]\n",
    "                # Mask it in train\n",
    "                train_matrix.at[user, item] = np.nan\n",
    "        else:\n",
    "            skipped_users += 1  # Opcional: para ver cuÃ¡ntos usuarios se omiten\n",
    "\n",
    "    # Guarda este fold\n",
    "    folds.append((train_matrix, test_matrix))\n",
    "\n",
    "    # Mostrar estadÃ­sticas\n",
    "    train_ratings = np.count_nonzero(~np.isnan(train_matrix.values))\n",
    "    test_ratings = np.count_nonzero(~np.isnan(test_matrix.values))\n",
    "    print(f\"âœ… Train ratings: {train_ratings}\")\n",
    "    print(f\"âœ… Test ratings: {test_ratings}\")\n",
    "    print(f\"ðŸš« Skipped test users (less than 5 ratings): {skipped_users}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd8723e",
   "metadata": {},
   "source": [
    "## Modelos bÃ¡sicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de61406e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_mean(train):\n",
    "    mean = np.nanmean(train.values)\n",
    "    return pd.DataFrame(mean, index=train.index, columns=train.columns)\n",
    "\n",
    "## Si alguno de los promedios da NaN (p.ej., Ã­tem sin ratings), usar el global mean\n",
    "def item_mean(train):\n",
    "    col_means = train.mean(axis=0, skipna=True)\n",
    "    global_mean = train.stack().mean()\n",
    "    col_means = col_means.fillna(global_mean)\n",
    "    return pd.DataFrame(np.tile(col_means.to_numpy(), (train.shape[0], 1)),\n",
    "                        index=train.index, columns=train.columns)\n",
    "def user_mean(train):\n",
    "    row_means = train.mean(axis=1, skipna=True)\n",
    "    global_mean = train.stack().mean()\n",
    "    row_means = row_means.fillna(global_mean)\n",
    "    return pd.DataFrame(np.tile(row_means.to_numpy().reshape(-1, 1), (1, train.shape[1])),\n",
    "                        index=train.index, columns=train.columns)\n",
    "\n",
    "\n",
    "def random_baseline(train):\n",
    "    min_rating = np.nanmin(train.values)\n",
    "    max_rating = np.nanmax(train.values)\n",
    "    rand_preds = np.random.uniform(min_rating, max_rating, size=train.shape)\n",
    "    return pd.DataFrame(rand_preds, index=train.index, columns=train.columns)\n",
    "\n",
    "def popularity_baseline(train):\n",
    "    item_popularity = np.sum(~np.isnan(train.values), axis=0)  # nÃºmero de ratings por Ã­tem\n",
    "    preds = np.full(train.shape, np.nan)  # empezamos con predicciones vacÃ­as\n",
    "\n",
    "    for u in range(train.shape[0]):\n",
    "        # Ãtems no vistos por el usuario\n",
    "        unseen_items = np.isnan(train.values[u])\n",
    "        # Asigna el score de popularidad a esos Ã­tems\n",
    "        preds[u, unseen_items] = item_popularity[unseen_items]\n",
    "\n",
    "    # Normaliza por conveniencia (entre 1 y 5 por ejemplo)\n",
    "    min_score = np.nanmin(preds)\n",
    "    max_score = np.nanmax(preds)\n",
    "    if max_score > min_score:\n",
    "        preds = 1 + 4 * (preds - min_score) / (max_score - min_score)  # normalizar a escala 1-5\n",
    "\n",
    "    return pd.DataFrame(preds, index=train.index, columns=train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf9ca26",
   "metadata": {},
   "source": [
    "EvaluaciÃ³n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ca051e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1 (Modelos bÃ¡sicos) =====\n",
      "\n",
      "===== Fold 2 (Modelos bÃ¡sicos) =====\n",
      "\n",
      "===== Fold 3 (Modelos bÃ¡sicos) =====\n",
      "\n",
      "===== Fold 4 (Modelos bÃ¡sicos) =====\n",
      "\n",
      "===== Fold 5 (Modelos bÃ¡sicos) =====\n",
      "\n",
      "ðŸ“Š RESULTADOS PROMEDIO (Modelos bÃ¡sicos):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Novelty</th>\n",
       "      <th>Serendipity</th>\n",
       "      <th>Diversity</th>\n",
       "      <th>nDCG@20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global Mean</td>\n",
       "      <td>1.131323</td>\n",
       "      <td>0.952467</td>\n",
       "      <td>3.315833</td>\n",
       "      <td>4.740408</td>\n",
       "      <td>0.425912</td>\n",
       "      <td>0.856680</td>\n",
       "      <td>0.002902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Item Mean</td>\n",
       "      <td>1.046458</td>\n",
       "      <td>0.835527</td>\n",
       "      <td>4.400417</td>\n",
       "      <td>6.991342</td>\n",
       "      <td>0.526997</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.006828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Popularity</td>\n",
       "      <td>1.855435</td>\n",
       "      <td>1.564943</td>\n",
       "      <td>3.831671</td>\n",
       "      <td>1.385641</td>\n",
       "      <td>0.207034</td>\n",
       "      <td>0.459176</td>\n",
       "      <td>0.137572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random [min,max]</td>\n",
       "      <td>1.710692</td>\n",
       "      <td>1.395519</td>\n",
       "      <td>3.326825</td>\n",
       "      <td>5.426521</td>\n",
       "      <td>0.287795</td>\n",
       "      <td>0.919987</td>\n",
       "      <td>0.006398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>User Mean</td>\n",
       "      <td>1.060559</td>\n",
       "      <td>0.849070</td>\n",
       "      <td>3.315833</td>\n",
       "      <td>4.740408</td>\n",
       "      <td>0.425912</td>\n",
       "      <td>0.856680</td>\n",
       "      <td>0.002902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model      RMSE       MAE  Relevance   Novelty  Serendipity  \\\n",
       "0       Global Mean  1.131323  0.952467   3.315833  4.740408     0.425912   \n",
       "1         Item Mean  1.046458  0.835527   4.400417  6.991342     0.526997   \n",
       "2        Popularity  1.855435  1.564943   3.831671  1.385641     0.207034   \n",
       "3  Random [min,max]  1.710692  1.395519   3.326825  5.426521     0.287795   \n",
       "4         User Mean  1.060559  0.849070   3.315833  4.740408     0.425912   \n",
       "\n",
       "   Diversity   nDCG@20  \n",
       "0   0.856680  0.002902  \n",
       "1   1.000000  0.006828  \n",
       "2   0.459176  0.137572  \n",
       "3   0.919987  0.006398  \n",
       "4   0.856680  0.002902  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_metrics_basic = []\n",
    "\n",
    "for fold_num, (train_df, test_df) in enumerate(folds, start=1):\n",
    "    print(f\"\\n===== Fold {fold_num} (Modelos bÃ¡sicos) =====\")\n",
    "    R_train = train_df.values\n",
    "    R_test  = test_df.values\n",
    "\n",
    "    baselines = [\n",
    "        (\"Global Mean\",     global_mean(train_df).values),\n",
    "        (\"User Mean\",       user_mean(train_df).values),\n",
    "        (\"Item Mean\",       item_mean(train_df).values),\n",
    "        (\"Random [min,max]\", random_baseline(train_df).values),\n",
    "        (\"Popularity\",      popularity_baseline(train_df).values),\n",
    "    ]\n",
    "\n",
    "    for name, R_pred in baselines:\n",
    "        metrics = evaluate_all_metrics(\n",
    "            R_train, R_test, R_pred, train_df,\n",
    "            topn_nov_rel=20, topn_div=5, k_ndcg=20\n",
    "        )\n",
    "        metrics.update({\n",
    "            \"Fold\": fold_num,\n",
    "            \"Model\": name,\n",
    "        })\n",
    "        all_metrics_basic.append(metrics)\n",
    "\n",
    "# ---------- Resultados agregados ----------\n",
    "basic_results_df = pd.DataFrame(all_metrics_basic)\n",
    "final_basic_results = (\n",
    "    basic_results_df\n",
    "    .groupby([\"Model\"], as_index=False)\n",
    "    .mean(numeric_only=True)\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š RESULTADOS PROMEDIO (Modelos bÃ¡sicos):\")\n",
    "display(final_basic_results[[\n",
    "    \"Model\", \"RMSE\", \"MAE\", \"Relevance\", \"Novelty\", \"Serendipity\", \"Diversity\", \"nDCG@20\"\n",
    "]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3416a461",
   "metadata": {},
   "source": [
    "## Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d21788e",
   "metadata": {},
   "source": [
    "#### Basado en Usuario"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d11b5a",
   "metadata": {},
   "source": [
    "##### Pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc54c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1 =====\n",
      "Computing user-user Pearson similarity...\n",
      "  -> k = 50\n",
      "  -> k = 150\n",
      "  -> k = 500\n",
      "  -> k = all\n",
      "\n",
      "===== Fold 2 =====\n",
      "Computing user-user Pearson similarity...\n",
      "  -> k = 50\n",
      "  -> k = 150\n",
      "  -> k = 500\n",
      "  -> k = all\n",
      "\n",
      "===== Fold 3 =====\n",
      "Computing user-user Pearson similarity...\n",
      "  -> k = 50\n",
      "  -> k = 150\n",
      "  -> k = 500\n",
      "  -> k = all\n",
      "\n",
      "===== Fold 4 =====\n",
      "Computing user-user Pearson similarity...\n",
      "  -> k = 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrari\\OneDrive\\Desktop\\TFG\\Sistemas de recomendaciÃ³n\\.venv\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:782: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> k = 150\n",
      "  -> k = 500\n",
      "  -> k = all\n",
      "\n",
      "===== Fold 5 =====\n",
      "Computing user-user Pearson similarity...\n",
      "  -> k = 50\n",
      "  -> k = 150\n",
      "  -> k = 500\n",
      "  -> k = all\n",
      "\n",
      "ðŸ“Š RESULTADOS PROMEDIO POR k:\n",
      "  k_neighbors      RMSE       MAE  Relevance   Novelty  Serendipity  \\\n",
      "0          50  0.979806  0.771537   3.750000  8.367351          1.0   \n",
      "1         150  0.974857  0.766930   3.500000  8.380335          1.0   \n",
      "2         500  0.976163  0.768558   3.333333  8.381510          1.0   \n",
      "3         all  0.976179  0.768585   3.333333  8.381514          1.0   \n",
      "\n",
      "   Diversity   nDCG@20  \n",
      "0   0.892753  0.000351  \n",
      "1   0.892751  0.000330  \n",
      "2   0.892749  0.000261  \n",
      "3   0.892749  0.000261  \n"
     ]
    }
   ],
   "source": [
    "# ================= CF: USER-BASED PEARSON =================\n",
    "\n",
    "\n",
    "def compute_similarity_matrix(R_train, user_means):\n",
    "    U, I = R_train.shape\n",
    "\n",
    "    rows, cols = np.where(~np.isnan(R_train))\n",
    "    vals = R_train[rows, cols].astype(np.float32)\n",
    "\n",
    "    # centered ratings (only where rating exists)\n",
    "    data = vals - user_means[rows].astype(np.float32)\n",
    "\n",
    "    C = sp.csr_matrix((data, (rows, cols)), shape=(U, I))\n",
    "    M = sp.csr_matrix((np.ones_like(data, dtype=np.float32), (rows, cols)), shape=(U, I))\n",
    "\n",
    "    # Numerator: sum over co-rated items of (r-mean_u)(r-mean_v)\n",
    "    N = (C @ C.T).toarray().astype(np.float32)\n",
    "\n",
    "    # S[u,v] = sum over co-rated items of (r_u-mean_u)^2\n",
    "    S = (C.multiply(C) @ M.T).toarray().astype(np.float32)\n",
    "\n",
    "    denom = np.sqrt(S * S.T)  # denom[u,v] = sqrt(S[u,v] * S[v,u])\n",
    "\n",
    "    sim = np.divide(N, denom, out=np.zeros_like(N), where=denom > 0)\n",
    "    np.fill_diagonal(sim, 0.0)\n",
    "    return sim\n",
    "\n",
    "\n",
    "def predict_user_ratings(u, R_train, similarity_matrix, user_means, k, rated_by_item):\n",
    "    \"\"\"\n",
    "    Predict missing ratings for user u using user-based CF with Pearson sim.\n",
    "    rated_by_item[j]: np.array of users that rated item j in train.\n",
    "    \"\"\"\n",
    "    num_items = R_train.shape[1]\n",
    "    preds = np.copy(R_train[u])\n",
    "    sims_u = similarity_matrix[u]\n",
    "    missing_items = np.where(np.isnan(R_train[u]))[0]\n",
    "    for j in missing_items:\n",
    "        if np.isnan(R_train[u, j]):\n",
    "            neighbors = rated_by_item[j]\n",
    "            if neighbors.size == 0:\n",
    "                preds[j] = user_means[u]\n",
    "                continue\n",
    "\n",
    "            neighbors = neighbors[neighbors != u]\n",
    "            if neighbors.size == 0:\n",
    "                preds[j] = user_means[u]\n",
    "                continue\n",
    "\n",
    "            sims = sims_u[neighbors]\n",
    "\n",
    "            # top-k neighbors by |similarity|\n",
    "            if k is not None and neighbors.size > k:\n",
    "                top_idx = np.argpartition(np.abs(sims), -k)[-k:]\n",
    "                neighbors = neighbors[top_idx]\n",
    "                sims = sims[top_idx]\n",
    "\n",
    "            num = np.sum(sims * (R_train[neighbors, j] - user_means[neighbors]))\n",
    "            denom = np.sum(np.abs(sims))\n",
    "\n",
    "            preds[j] = user_means[u] if denom == 0 else user_means[u] + num / denom\n",
    "\n",
    "    return preds\n",
    "\n",
    "# ================= 5-FOLD EVAL LOOP =================\n",
    "\n",
    "k_values = [50, 150, 500, None]  # None -> all neighbors\n",
    "all_folds_metrics = []\n",
    "\n",
    "for fold_num, (train_matrix, test_matrix) in enumerate(folds, start=1):\n",
    "    print(f\"\\n===== Fold {fold_num} =====\")\n",
    "\n",
    "    R_train = train_matrix.values\n",
    "    R_test = test_matrix.values\n",
    "    num_users, num_items = R_train.shape\n",
    "\n",
    "    # User means\n",
    "    user_means = np.array([\n",
    "        np.nanmean(R_train[u]) if np.any(~np.isnan(R_train[u])) else global_mean\n",
    "        for u in range(num_users)\n",
    "    ])\n",
    "\n",
    "    # User-user Pearson similarity (once per fold)\n",
    "    print(\"Computing user-user Pearson similarity...\")\n",
    "    sim_matrix = compute_similarity_matrix(R_train, user_means)\n",
    "\n",
    "    # For each item, which users rated it (for fast neighbor lookup)\n",
    "    rated_by_item = [\n",
    "        np.where(~np.isnan(R_train[:, j]))[0]\n",
    "        for j in range(num_items)\n",
    "    ]\n",
    "\n",
    "   \n",
    "\n",
    "    for k in k_values:\n",
    "        print(f\"  -> k = {k if k is not None else 'all'}\")\n",
    "\n",
    "        # Predict for all users\n",
    "        R_pred_rows = Parallel(n_jobs=-1)(\n",
    "            delayed(predict_user_ratings)(\n",
    "                u, R_train, sim_matrix, user_means, k, rated_by_item\n",
    "            )\n",
    "            for u in range(num_users)\n",
    "        )\n",
    "        R_pred = np.vstack(R_pred_rows)\n",
    "\n",
    "        # Evaluate with shared metric functions\n",
    "        metrics = evaluate_all_metrics(\n",
    "            R_train, R_test, R_pred, train_df,\n",
    "            topn_nov_rel=20,\n",
    "            topn_div=5,\n",
    "            k_ndcg=20\n",
    "        )\n",
    "\n",
    "        metrics.update({\n",
    "            \"Fold\": fold_num,\n",
    "            \"k_neighbors\": \"all\" if k is None else k,\n",
    "            \"Model\": \"Memory-based CF\",\n",
    "            \"Similarity\": \"Pearson (user-based)\"\n",
    "        })\n",
    "\n",
    "        all_folds_metrics.append(metrics)\n",
    "\n",
    "# ================= AGGREGATE RESULTS =================\n",
    "\n",
    "results_df = pd.DataFrame(all_folds_metrics)\n",
    "\n",
    "final_results = (\n",
    "    results_df\n",
    "    .groupby([\"k_neighbors\", \"Model\", \"Similarity\"], as_index=False)\n",
    "    .mean(numeric_only=True)\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š RESULTADOS PROMEDIO POR k:\")\n",
    "print(\n",
    "    final_results[\n",
    "        [\"k_neighbors\", \"RMSE\", \"MAE\",\n",
    "         \"Relevance\", \"Novelty\", \"Serendipity\",\n",
    "         \"Diversity\", \"nDCG@20\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9972e4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k_neighbors</th>\n",
       "      <th>Model</th>\n",
       "      <th>Similarity</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Novelty</th>\n",
       "      <th>Relevance</th>\n",
       "      <th>Serendipity</th>\n",
       "      <th>Diversity</th>\n",
       "      <th>nDCG@20</th>\n",
       "      <th>Fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>Memory-based CF</td>\n",
       "      <td>Pearson (user-based)</td>\n",
       "      <td>0.979806</td>\n",
       "      <td>0.771537</td>\n",
       "      <td>8.367351</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.892753</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150</td>\n",
       "      <td>Memory-based CF</td>\n",
       "      <td>Pearson (user-based)</td>\n",
       "      <td>0.974857</td>\n",
       "      <td>0.766930</td>\n",
       "      <td>8.380335</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.892751</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>Memory-based CF</td>\n",
       "      <td>Pearson (user-based)</td>\n",
       "      <td>0.976163</td>\n",
       "      <td>0.768558</td>\n",
       "      <td>8.381510</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.892749</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>all</td>\n",
       "      <td>Memory-based CF</td>\n",
       "      <td>Pearson (user-based)</td>\n",
       "      <td>0.976179</td>\n",
       "      <td>0.768585</td>\n",
       "      <td>8.381514</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.892749</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  k_neighbors            Model            Similarity      RMSE       MAE  \\\n",
       "0          50  Memory-based CF  Pearson (user-based)  0.979806  0.771537   \n",
       "1         150  Memory-based CF  Pearson (user-based)  0.974857  0.766930   \n",
       "2         500  Memory-based CF  Pearson (user-based)  0.976163  0.768558   \n",
       "3         all  Memory-based CF  Pearson (user-based)  0.976179  0.768585   \n",
       "\n",
       "    Novelty  Relevance  Serendipity  Diversity   nDCG@20  Fold  \n",
       "0  8.367351   3.750000          1.0   0.892753  0.000351   3.0  \n",
       "1  8.380335   3.500000          1.0   0.892751  0.000330   3.0  \n",
       "2  8.381510   3.333333          1.0   0.892749  0.000261   3.0  \n",
       "3  8.381514   3.333333          1.0   0.892749  0.000261   3.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "item-based-cf",
   "metadata": {},
   "source": [
    "#### Basado en Ãtems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "item-based-cosine",
   "metadata": {},
   "source": [
    "##### Coseno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "item-cosine-funcs",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_item_similarity_cosine(R_train):\n",
    "    R_filled = np.nan_to_num(R_train, nan=0.0)\n",
    "    return cosine_similarity(R_filled.T)\n",
    "\n",
    "def predict_all_users_item_based(R_train, item_sim, k=None):\n",
    "    num_users, num_items = R_train.shape\n",
    "    out = np.copy(R_train)\n",
    "    for u in range(num_users):\n",
    "        rated = ~np.isnan(R_train[u])\n",
    "        rated_idx = np.where(rated)[0]\n",
    "        rated_r = R_train[u, rated_idx]\n",
    "        for j in range(num_items):\n",
    "            if np.isnan(R_train[u, j]):\n",
    "                sims = item_sim[j, rated_idx]\n",
    "                if rated_idx.size == 0:\n",
    "                    out[u, j] = np.nanmean(R_train[:, j]) if np.any(~np.isnan(R_train[:, j])) else 0.0\n",
    "                    continue\n",
    "                if k is not None and rated_idx.size > k:\n",
    "                    top_idx = np.argpartition(np.abs(sims), -k)[-k:]\n",
    "                    sims_k = sims[top_idx]\n",
    "                    rr = rated_r[top_idx]\n",
    "                else:\n",
    "                    sims_k = sims\n",
    "                    rr = rated_r\n",
    "                denom = np.sum(np.abs(sims_k))\n",
    "                out[u, j] = (np.sum(sims_k * rr) / denom) if denom > 0 else (np.nanmean(R_train[:, j]) if np.any(~np.isnan(R_train[:, j])) else 0.0)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "item-cosine-run",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Fold 1 (Item-based) =====\n",
      "Computing item-item cosine similarity...\n",
      "  -> k = 50\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "evaluate_all_metrics() got multiple values for argument 'topn_nov_rel'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  -> k = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mk\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mis\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mall\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m R_pred = predict_all_users_item_based(R_train, item_sim, k)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m metrics = \u001b[43mevaluate_all_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mR_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtopn_nov_rel\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtopn_div\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_ndcg\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\n\u001b[32m     23\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m metrics.update({\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mFold\u001b[39m\u001b[33m\"\u001b[39m: fold_num,\n\u001b[32m     27\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mk_neighbors\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mall\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m k,\n\u001b[32m     28\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mMemory-based CF\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     29\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSimilarity\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mCosine (item-based)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m })\n\u001b[32m     32\u001b[39m all_folds_metrics_item.append(metrics)\n",
      "\u001b[31mTypeError\u001b[39m: evaluate_all_metrics() got multiple values for argument 'topn_nov_rel'"
     ]
    }
   ],
   "source": [
    "# EvaluaciÃ³n 5-fold para CF basado en Ã­tems (coseno)\n",
    "k_values = [50, 150, 500, None]  # None -> todos los vecinos\n",
    "all_folds_metrics_item = []\n",
    "\n",
    "\n",
    "for fold_num, (train_matrix, test_matrix) in enumerate(folds, start=1):\n",
    "    print(f\"\\n===== Fold {fold_num} (Item-based) =====\")\n",
    "    R_train = train_matrix.values\n",
    "    R_test = test_matrix.values\n",
    "\n",
    "    print(\"Computing item-item cosine similarity...\")\n",
    "    item_sim = compute_item_similarity_cosine(R_train)\n",
    "\n",
    "    for k in k_values:\n",
    "        print(f\"  -> k = {k if k is not None else 'all'}\")\n",
    "        R_pred = predict_all_users_item_based(R_train, item_sim, k)\n",
    "\n",
    "        metrics = evaluate_all_metrics(\n",
    "            R_train, R_test, R_pred, train_df,\n",
    "            topn_nov_rel=20,\n",
    "            topn_div=5,\n",
    "            k_ndcg=20\n",
    "        )\n",
    "\n",
    "\n",
    "        metrics.update({\n",
    "            \"Fold\": fold_num,\n",
    "            \"k_neighbors\": \"all\" if k is None else k,\n",
    "            \"Model\": \"Memory-based CF\",\n",
    "            \"Similarity\": \"Cosine (item-based)\"\n",
    "        })\n",
    "\n",
    "        all_folds_metrics_item.append(metrics)\n",
    "\n",
    "results_item_df = pd.DataFrame(all_folds_metrics_item)\n",
    "final_results_item = (\n",
    "    results_item_df\n",
    "    .groupby([\"k_neighbors\", \"Model\", \"Similarity\"], as_index=False)\n",
    "    .mean(numeric_only=True)\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ“Š RESULTADOS PROMEDIO POR k (Item-based):\")\n",
    "display(final_results_item[[\"k_neighbors\", \"RMSE\", \"MAE\",\n",
    "                           \"Relevance\", \"Novelty\", \"Serendipity\",\n",
    "                           \"Diversity\", \"nDCG@20\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b193c6",
   "metadata": {},
   "source": [
    "### Modelos generales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e996f7",
   "metadata": {},
   "source": [
    "#### Modelos Factoriales Latentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fe1371",
   "metadata": {},
   "source": [
    "##### SVD++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5526a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mf_fit_biased(R_train,\n",
    "                  n_factors=40,\n",
    "                  n_epochs=20,\n",
    "                  lr=0.01,\n",
    "                  reg=0.05,\n",
    "                  random_state=42,\n",
    "                  verbose=False):\n",
    "    \"\"\"\n",
    "    Biased MF: r_hat = mu + b_u + b_i + p_u^T q_i\n",
    "    R_train: matriz numpy (usuarios x Ã­tems) con NaNs en celdas desconocidas.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    R = R_train.astype(float)\n",
    "    num_users, num_items = R.shape\n",
    "\n",
    "    mask = ~np.isnan(R)\n",
    "    u_idx, i_idx = np.where(mask)\n",
    "    ratings = R[mask]\n",
    "    if ratings.size == 0:\n",
    "        raise ValueError(\"R_train no tiene ratings conocidos.\")\n",
    "\n",
    "    mu = float(np.mean(ratings))\n",
    "    bu = np.zeros(num_users, dtype=float)\n",
    "    bi = np.zeros(num_items, dtype=float)\n",
    "    P = 0.1 * rng.standard_normal((num_users, n_factors))\n",
    "    Q = 0.1 * rng.standard_normal((num_items, n_factors))\n",
    "\n",
    "    n_obs = ratings.shape[0]\n",
    "    indices = np.arange(n_obs)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rng.shuffle(indices)\n",
    "        for k in indices:\n",
    "            u = u_idx[k]\n",
    "            i = i_idx[k]\n",
    "            r_ui = ratings[k]\n",
    "\n",
    "            pred = mu + bu[u] + bi[i] + P[u] @ Q[i]\n",
    "            err = r_ui - pred\n",
    "\n",
    "            bu[u] += lr * (err - reg * bu[u])\n",
    "            bi[i] += lr * (err - reg * bi[i])\n",
    "\n",
    "            p_u = P[u]\n",
    "            q_i = Q[i]\n",
    "\n",
    "            P[u] += lr * (err * q_i - reg * p_u)\n",
    "            Q[i] += lr * (err * p_u - reg * q_i)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs} terminada\")\n",
    "\n",
    "    return mu, bu, bi, P, Q\n",
    "\n",
    "\n",
    "def mf_predict_unknown(mu, bu, bi, P, Q, R_train):\n",
    "    \"\"\"\n",
    "    Devuelve matriz R_pred con NaN en celdas conocidas y predicciones en desconocidas.\n",
    "    \"\"\"\n",
    "    R = R_train.astype(float)\n",
    "    num_users, num_items = R.shape\n",
    "\n",
    "    full_pred = mu + bu[:, None] + bi[None, :] + P @ Q.T\n",
    "\n",
    "    R_pred = np.full((num_users, num_items), np.nan, dtype=float)\n",
    "    mask_unknown = np.isnan(R)\n",
    "    R_pred[mask_unknown] = full_pred[mask_unknown]\n",
    "    return R_pred\n",
    "\n",
    "\n",
    "def evaluate_mf_with_metrics_on_folds(\n",
    "    folds,\n",
    "    n_factors=40,\n",
    "    n_epochs=20,\n",
    "    lr=0.01,\n",
    "    reg=0.05,\n",
    "    random_state=42,\n",
    "    topn_nov_rel=20,\n",
    "    topn_div=5,\n",
    "    k_ndcg=20,\n",
    "    max_folds=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    rows = []\n",
    "\n",
    "    for f, (train_df, test_df) in enumerate(folds, 1):\n",
    "        if max_folds is not None and f > max_folds:\n",
    "            break\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n=== Fold {f} ===\")\n",
    "\n",
    "        # Muy importante: actualizar las globales que usa eval_diversity\n",
    "        global R_train, R_test\n",
    "        R_train = train_df.values.astype(float)\n",
    "        R_test = test_df.values.astype(float)\n",
    "\n",
    "        mu, bu, bi, P, Q = mf_fit_biased(\n",
    "            R_train,\n",
    "            n_factors=n_factors,\n",
    "            n_epochs=n_epochs,\n",
    "            lr=lr,\n",
    "            reg=reg,\n",
    "            random_state=random_state + f,  # semilla distinta por fold\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        R_pred = mf_predict_unknown(mu, bu, bi, P, Q, R_train)\n",
    "\n",
    "        metrics = evaluate_all_metrics(\n",
    "            R_train, R_test, R_pred, train_df,\n",
    "            topn_nov_rel=topn_nov_rel,\n",
    "            topn_div=topn_div,\n",
    "            k_ndcg=k_ndcg,\n",
    "        )\n",
    "        metrics[\"fold\"] = f\n",
    "        rows.append(metrics)\n",
    "\n",
    "    metrics_df = pd.DataFrame(rows)\n",
    "    cols = [\"fold\"] + [c for c in metrics_df.columns if c != \"fold\"]\n",
    "    metrics_df = metrics_df[cols]\n",
    "    avg_metrics = metrics_df.drop(columns=[\"fold\"]).mean(numeric_only=True).to_dict()\n",
    "    return metrics_df, avg_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "020a112f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fold      RMSE       MAE   Novelty  Relevance  Serendipity  Diversity  \\\n",
      "0     1  0.944720  0.743779  3.034728   4.382184          NaN   0.704821   \n",
      "1     2  0.978627  0.763278  2.989539   4.342199          NaN   0.670113   \n",
      "2     3  0.921250  0.730451  3.044235   4.575269          NaN   0.701794   \n",
      "3     4  0.921644  0.733530  3.095201   4.329932          NaN   0.707303   \n",
      "4     5  0.961670  0.749913  2.989074   4.283688     0.432525   0.684667   \n",
      "\n",
      "    nDCG@20  \n",
      "0  0.057466  \n",
      "1  0.044046  \n",
      "2  0.061672  \n",
      "3  0.054313  \n",
      "4  0.044704  \n",
      "Promedio MF: {'RMSE': 0.9455821142831233, 'MAE': 0.7441903939920028, 'Novelty': 3.03055541644787, 'Relevance': 4.382654244572418, 'Serendipity': 0.43252457384596243, 'Diversity': 0.6937396518160441, 'nDCG@20': 0.05244020634828974}\n"
     ]
    }
   ],
   "source": [
    "mf_metrics_df, mf_avg_metrics = evaluate_mf_with_metrics_on_folds(\n",
    "    folds,\n",
    "    n_factors=40,\n",
    "    n_epochs=20,\n",
    "    lr=0.01,\n",
    "    reg=0.05,\n",
    "    random_state=42,\n",
    "    topn_nov_rel=20,\n",
    "    topn_div=5,\n",
    "    k_ndcg=20,\n",
    "    max_folds=None,   # o 1â€“2 mientras pruebas\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "print(mf_metrics_df)\n",
    "print(\"Promedio MF:\", mf_avg_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e7a00c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def svdpp_fit(R_train,\n",
    "              n_factors=40,\n",
    "              n_epochs=20,\n",
    "              lr=0.01,\n",
    "              reg=0.05,\n",
    "              random_state=42,\n",
    "              use_implicit=True,\n",
    "              max_items_per_user=None,\n",
    "              verbose=False):\n",
    "    \"\"\"\n",
    "    SVD++:\n",
    "      r_hat_ui = mu + b_u + b_i + q_i^T ( p_u + |N(u)|^{-1/2} sum_{j in N(u)} y_j )\n",
    "\n",
    "    R_train: matriz numpy (usuarios x Ã­tems) con NaNs en celdas desconocidas.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    R = R_train.astype(np.float32)\n",
    "    num_users, num_items = R.shape\n",
    "\n",
    "    mask = ~np.isnan(R)\n",
    "    u_idx, i_idx = np.where(mask)\n",
    "    ratings = R[mask]\n",
    "    if ratings.size == 0:\n",
    "        raise ValueError(\"R_train no tiene ratings conocidos.\")\n",
    "\n",
    "    # Global mean y parÃ¡metros\n",
    "    mu = float(ratings.mean())\n",
    "    bu = np.zeros(num_users, dtype=np.float32)\n",
    "    bi = np.zeros(num_items, dtype=np.float32)\n",
    "\n",
    "    P = 0.1 * rng.standard_normal((num_users, n_factors)).astype(np.float32)  # user factors\n",
    "    Q = 0.1 * rng.standard_normal((num_items, n_factors)).astype(np.float32)  # item factors\n",
    "    Y = 0.1 * rng.standard_normal((num_items, n_factors)).astype(np.float32)  # implicit item factors\n",
    "\n",
    "    # N(u): Ã­tems valorados por cada usuario (para el tÃ©rmino implÃ­cito)\n",
    "    user_items = [list(np.where(mask[u])[0]) for u in range(num_users)]\n",
    "\n",
    "    n_obs = ratings.shape[0]\n",
    "    indices = np.arange(n_obs)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rng.shuffle(indices)\n",
    "        se = 0.0\n",
    "\n",
    "        for k in indices:\n",
    "            u = u_idx[k]\n",
    "            i = i_idx[k]\n",
    "            r_ui = ratings[k]\n",
    "\n",
    "            Nu = user_items[u]\n",
    "            if max_items_per_user is not None and len(Nu) > max_items_per_user:\n",
    "                # para aligerar, nos quedamos con un subconjunto\n",
    "                Nu = Nu[-max_items_per_user:]\n",
    "\n",
    "            # Vector de usuario efectivo p_u + |N(u)|^-1/2 sum y_j\n",
    "            if use_implicit and len(Nu) > 0:\n",
    "                norm = 1.0 / np.sqrt(len(Nu))\n",
    "                y_sum = Y[Nu].sum(axis=0) * norm\n",
    "                user_vec = P[u] + y_sum\n",
    "            else:\n",
    "                norm = 0.0  # no se usa\n",
    "                user_vec = P[u]\n",
    "\n",
    "            # PredicciÃ³n y error\n",
    "            pred = mu + bu[u] + bi[i] + np.dot(Q[i], user_vec)\n",
    "            err = r_ui - pred\n",
    "            se += err * err\n",
    "\n",
    "            # Actualizar sesgos\n",
    "            bu[u] += lr * (err - reg * bu[u])\n",
    "            bi[i] += lr * (err - reg * bi[i])\n",
    "\n",
    "            # Guardar copias para las actualizaciones\n",
    "            p_u = P[u].copy()\n",
    "            q_i = Q[i].copy()\n",
    "\n",
    "            # Actualizar factores latentes\n",
    "            P[u] += lr * (err * q_i - reg * p_u)\n",
    "            Q[i] += lr * (err * user_vec - reg * q_i)\n",
    "\n",
    "            # Actualizar factores implÃ­citos Y_j\n",
    "            if use_implicit and len(Nu) > 0:\n",
    "                # broadcast: cada Y[j] se mueve en la misma direcciÃ³n\n",
    "                Y[Nu] += lr * (err * norm * q_i - reg * Y[Nu])\n",
    "\n",
    "        if verbose:\n",
    "            rmse = np.sqrt(se / n_obs)\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs} â€” train RMSE: {rmse:.4f}\")\n",
    "\n",
    "    # devolvemos tambiÃ©n user_items para usar N(u) en la fase de predicciÃ³n\n",
    "    return mu, bu, bi, P, Q, Y, user_items\n",
    "def svdpp_predict_unknown(mu, bu, bi, P, Q, Y, user_items, R_train,\n",
    "                          use_implicit=True,\n",
    "                          max_items_per_user=None):\n",
    "    \"\"\"\n",
    "    Devuelve matriz R_pred con NaN en celdas conocidas y predicciones en desconocidas,\n",
    "    usando la fÃ³rmula de SVD++.\n",
    "    \"\"\"\n",
    "    R = R_train.astype(float)\n",
    "    num_users, num_items = R.shape\n",
    "\n",
    "    full_pred = np.zeros((num_users, num_items), dtype=float)\n",
    "\n",
    "    for u in range(num_users):\n",
    "        Nu = user_items[u]\n",
    "        if max_items_per_user is not None and len(Nu) > max_items_per_user:\n",
    "            Nu = Nu[-max_items_per_user:]\n",
    "\n",
    "        # vector de usuario efectivo\n",
    "        if use_implicit and len(Nu) > 0:\n",
    "            norm = 1.0 / np.sqrt(len(Nu))\n",
    "            y_sum = Y[Nu].sum(axis=0) * norm\n",
    "            user_vec = P[u] + y_sum\n",
    "        else:\n",
    "            user_vec = P[u]\n",
    "\n",
    "        # Î¼ + b_u + b_i + q_i^T user_vec  para todos los Ã­tems de golpe\n",
    "        full_pred[u, :] = mu + bu[u] + bi + Q @ user_vec\n",
    "\n",
    "    R_pred = np.full((num_users, num_items), np.nan, dtype=float)\n",
    "    mask_unknown = np.isnan(R)\n",
    "    R_pred[mask_unknown] = full_pred[mask_unknown]\n",
    "    return R_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25df6468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def svdpp_fit_fast(\n",
    "    R_train,\n",
    "    n_factors=40,\n",
    "    n_epochs=20,\n",
    "    lr=0.01,\n",
    "    reg=0.05,\n",
    "    random_state=42,\n",
    "    use_implicit=True,\n",
    "    max_items_per_user=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    SVD++ (versiÃ³n rÃ¡pida):\n",
    "      - Recorre por usuario\n",
    "      - Cachea y_sum = |N(u)|^-1/2 * sum_{j in N(u)} Y[j] una vez por usuario\n",
    "      - Actualiza Y de forma agregada (1 vez por usuario y Ã©poca)\n",
    "      - Si max_items_per_user: submuestreo aleatorio sin sesgo (NO \"los Ãºltimos\")\n",
    "\n",
    "    R_train: np.array (U x I) con NaNs en desconocidos.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    R = R_train.astype(np.float32)\n",
    "    num_users, num_items = R.shape\n",
    "\n",
    "    mask = ~np.isnan(R)\n",
    "    ratings = R[mask]\n",
    "    if ratings.size == 0:\n",
    "        raise ValueError(\"R_train no tiene ratings conocidos.\")\n",
    "\n",
    "    mu = float(ratings.mean())\n",
    "    bu = np.zeros(num_users, dtype=np.float32)\n",
    "    bi = np.zeros(num_items, dtype=np.float32)\n",
    "\n",
    "    P = (0.1 * rng.standard_normal((num_users, n_factors))).astype(np.float32)\n",
    "    Q = (0.1 * rng.standard_normal((num_items, n_factors))).astype(np.float32)\n",
    "    Y = (0.1 * rng.standard_normal((num_items, n_factors))).astype(np.float32)\n",
    "\n",
    "    # Para cada usuario, guardamos los Ã­tems valorados y sus ratings (para iterar rÃ¡pido)\n",
    "    user_items_full = []\n",
    "    user_ratings_full = []\n",
    "    for u in range(num_users):\n",
    "        items_u = np.where(mask[u])[0]\n",
    "        user_items_full.append(items_u.astype(np.int32))\n",
    "        user_ratings_full.append(R[u, items_u].astype(np.float32))\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        users = np.arange(num_users)\n",
    "        rng.shuffle(users)\n",
    "\n",
    "        se = 0.0\n",
    "        n_obs = 0\n",
    "\n",
    "        for u in users:\n",
    "            items_u = user_items_full[u]\n",
    "            if items_u.size == 0:\n",
    "                continue\n",
    "\n",
    "            # --------- tÃ©rmino implÃ­cito (cacheado una vez por usuario) ---------\n",
    "            Nu = items_u\n",
    "            y_sum = 0.0  # vector o escalar segÃºn caso\n",
    "            norm = 0.0\n",
    "\n",
    "            if use_implicit and Nu.size > 0:\n",
    "                if max_items_per_user is not None and Nu.size > max_items_per_user:\n",
    "                    # submuestreo aleatorio SIN sesgo por id\n",
    "                    Nu = rng.choice(Nu, size=max_items_per_user, replace=False)\n",
    "                norm = (1.0 / np.sqrt(Nu.size)).astype(np.float32)\n",
    "                y_sum = Y[Nu].sum(axis=0) * norm  # (n_factors,)\n",
    "\n",
    "            # Acumulador del gradiente para Y (una sola actualizaciÃ³n al final)\n",
    "            grad_y = np.zeros(n_factors, dtype=np.float32)\n",
    "\n",
    "            # --------- iteramos ratings del usuario u ---------\n",
    "            # (explicit: recorremos todos los ratings observados del usuario)\n",
    "            for i, r_ui in zip(items_u, user_ratings_full[u]):\n",
    "                user_vec = P[u] + (y_sum if (use_implicit and Nu.size > 0) else 0.0)\n",
    "\n",
    "                pred = mu + bu[u] + bi[i] + np.dot(Q[i], user_vec)\n",
    "                err = r_ui - pred\n",
    "\n",
    "                se += float(err * err)\n",
    "                n_obs += 1\n",
    "\n",
    "                # biases\n",
    "                bu[u] += lr * (err - reg * bu[u])\n",
    "                bi[i] += lr * (err - reg * bi[i])\n",
    "\n",
    "                # copias para gradientes consistentes\n",
    "                p_u_old = P[u].copy()\n",
    "                q_i_old = Q[i].copy()\n",
    "\n",
    "                # factores\n",
    "                P[u] += lr * (err * q_i_old - reg * p_u_old)\n",
    "                Q[i] += lr * (err * user_vec - reg * q_i_old)\n",
    "\n",
    "                # acumulamos contribuciÃ³n a Y (evita tocar |Nu| filas aquÃ­)\n",
    "                if use_implicit and Nu.size > 0:\n",
    "                    grad_y += err * q_i_old\n",
    "\n",
    "            # --------- actualizaciÃ³n agregada de Y (1 vez por usuario) ---------\n",
    "            if use_implicit and Nu.size > 0:\n",
    "                # Ajuste: el tÃ©rmino de regularizaciÃ³n en tu versiÃ³n se aplica \"por rating\".\n",
    "                # AquÃ­ lo aproximamos escalÃ¡ndolo por el nÂº de ratings del usuario en esta Ã©poca.\n",
    "                m = items_u.size  # nÂº de observaciones del usuario (aprox. \"por rating\")\n",
    "                Y[Nu] += lr * (norm * grad_y - (reg * m) * Y[Nu])\n",
    "\n",
    "        if verbose and n_obs > 0:\n",
    "            rmse = np.sqrt(se / n_obs)\n",
    "            print(f\"Epoch {epoch+1}/{n_epochs} â€” train RMSE: {rmse:.4f}\")\n",
    "\n",
    "    # devolvemos user_items_full (historial completo) para predicciÃ³n\n",
    "    return mu, bu, bi, P, Q, Y, user_items_full\n",
    "\n",
    "\n",
    "def svdpp_predict_unknown(\n",
    "    mu, bu, bi, P, Q, Y, user_items, R_train,\n",
    "    use_implicit=True,\n",
    "    max_items_per_user=None,\n",
    "    random_state=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Predice SOLO en celdas desconocidas (NaN), dejando NaN en las conocidas.\n",
    "    Si max_items_per_user: usa submuestreo determinista por usuario (para que no \"baile\").\n",
    "    \"\"\"\n",
    "    R = R_train.astype(float)\n",
    "    num_users, num_items = R.shape\n",
    "\n",
    "    full_pred = np.zeros((num_users, num_items), dtype=float)\n",
    "\n",
    "    for u in range(num_users):\n",
    "        Nu = user_items[u]\n",
    "        if use_implicit and Nu.size > 0:\n",
    "            if max_items_per_user is not None and Nu.size > max_items_per_user:\n",
    "                # determinista por usuario para estabilidad\n",
    "                rng_u = np.random.default_rng(random_state + int(u))\n",
    "                Nu_eff = rng_u.choice(Nu, size=max_items_per_user, replace=False)\n",
    "            else:\n",
    "                Nu_eff = Nu\n",
    "\n",
    "            norm = 1.0 / np.sqrt(Nu_eff.size)\n",
    "            y_sum = Y[Nu_eff].sum(axis=0) * norm\n",
    "            user_vec = P[u] + y_sum\n",
    "        else:\n",
    "            user_vec = P[u]\n",
    "\n",
    "        full_pred[u, :] = mu + bu[u] + bi + (Q @ user_vec)\n",
    "\n",
    "    R_pred = np.full((num_users, num_items), np.nan, dtype=float)\n",
    "    mask_unknown = np.isnan(R)\n",
    "    R_pred[mask_unknown] = full_pred[mask_unknown]\n",
    "    return R_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad7e734f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RMSE': 0.9917634227098485, 'MAE': 0.7881659490046544, 'Novelty': 2.2303338300469955, 'Relevance': 4.243490247678328, 'Serendipity': 0.379919028340081, 'Diversity': 0.5637153834420554, 'nDCG@20': 0.05754468565112882}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_svdpp_with_metrics_on_folds(\n",
    "    folds,\n",
    "    n_factors=40,\n",
    "    n_epochs=20,\n",
    "    lr=0.01,\n",
    "    reg=0.05,\n",
    "    random_state=42,\n",
    "    topn_nov_rel=20,\n",
    "    topn_div=5,\n",
    "    k_ndcg=20,\n",
    "    max_folds=None,\n",
    "    use_implicit=True,\n",
    "    max_items_per_user=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    rows = []\n",
    "\n",
    "    for f, (train_df, test_df) in enumerate(folds, 1):\n",
    "        if max_folds is not None and f > max_folds:\n",
    "            break\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n=== Fold {f} ===\")\n",
    "\n",
    "        # Muy importante: actualizar las globales que usa eval_diversity\n",
    "        global R_train, R_test\n",
    "        R_train = train_df.values.astype(float)\n",
    "        R_test = test_df.values.astype(float)\n",
    "\n",
    "        mu, bu, bi, P, Q, Y, user_items = svdpp_fit_fast(\n",
    "            R_train,\n",
    "            n_factors=n_factors,\n",
    "            n_epochs=n_epochs,\n",
    "            lr=lr,\n",
    "            reg=reg,\n",
    "            random_state=random_state + f,\n",
    "            use_implicit=use_implicit,\n",
    "            max_items_per_user=max_items_per_user,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "        R_pred = svdpp_predict_unknown(\n",
    "            mu, bu, bi, P, Q, Y, user_items, R_train,\n",
    "            use_implicit=use_implicit,\n",
    "            max_items_per_user=max_items_per_user,\n",
    "        )\n",
    "\n",
    "        metrics = evaluate_all_metrics(\n",
    "            R_train, R_test, R_pred, train_df,\n",
    "            topn_nov_rel=topn_nov_rel,\n",
    "            topn_div=topn_div,\n",
    "            k_ndcg=k_ndcg,\n",
    "        )\n",
    "        metrics[\"fold\"] = f\n",
    "        rows.append(metrics)\n",
    "\n",
    "    metrics_df = pd.DataFrame(rows)\n",
    "    cols = [\"fold\"] + [c for c in metrics_df.columns if c != \"fold\"]\n",
    "    metrics_df = metrics_df[cols]\n",
    "    avg_metrics = metrics_df.drop(columns=[\"fold\"]).mean(numeric_only=True).to_dict()\n",
    "    return metrics_df, avg_metrics\n",
    "\n",
    "metrics_df, avg_metrics = evaluate_svdpp_with_metrics_on_folds(\n",
    "    folds,\n",
    "    n_factors=40,\n",
    "    n_epochs=20,\n",
    "    lr=0.001,\n",
    "    reg=0.05,\n",
    "    use_implicit=True\n",
    "    ,          # SVD++\n",
    "    max_items_per_user=20,      # para aligerar\n",
    "    verbose=False\n",
    ")\n",
    "print(avg_metrics)     # promedio sobre folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f344eab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
